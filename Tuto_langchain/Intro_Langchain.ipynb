{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RlBQG7pL2r0"
      },
      "source": [
        "Article : https://larevueia.fr/langchain-le-guide-essentiel/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f30pTxEr66rF"
      },
      "source": [
        "# Concept 1 : les prompts dans Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seHkWvh46q4z"
      },
      "outputs": [],
      "source": [
        "%pip install langchain langchain-openai langchain-chroma beautifulsoup4 langchain-community langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QAqsjQm6yjb"
      },
      "source": [
        "## Syntaxe LCEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXO9_zNC60ao",
        "outputId": "b600d650-cc13-4821-eed4-34d9eb91fd58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L’Argentine. Elle a battu la France en finale (3-3, 4-2 t.a.b.) le 18 décembre 2022.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# 1. Configuration de la clé API (Variable d'environnement)\n",
        "load_dotenv()\n",
        "\n",
        "# 2. Création du Modèle (ChatModel)\n",
        "# Nous utilisons ChatOpenAI qui est optimisé pour la conversation\n",
        "llm = ChatOpenAI(model_name=\"gpt-5\")\n",
        "\n",
        "# 3. Création du Template\n",
        "template = \"\"\"\n",
        "Question de l'utilisateur : {question}\n",
        "Réponse :\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# 4. Création de la Chaîne (Syntaxe LCEL)\n",
        "# C'est ici que la magie opère : Prompt -> Modèle\n",
        "chain = prompt | llm\n",
        "\n",
        "# 5. Exécution\n",
        "reponse = chain.invoke({\"question\": \"Qui a gagné la coupe du monde 2022 au Qatar ?\"})\n",
        "print(reponse.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UojOidS607k"
      },
      "source": [
        "## Prompt avec contexte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ypqgmopg7LDY",
        "outputId": "060745fe-e9d2-424d-891e-099a3cf2033b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Harrison Chase.\n"
          ]
        }
      ],
      "source": [
        "template_contexte = \"\"\"\n",
        "Utilisez uniquement le contexte suivant pour répondre à la question.\n",
        "Si vous ne connaissez pas la réponse d'après le contexte, dites \"Je ne sais pas\".\n",
        "\n",
        "Contexte :\n",
        "LangChain est un framework open-source lancé en 2022 par Harrison Chase.\n",
        "Il permet de connecter des LLM à des sources de données externes.\n",
        "En 2025, la fonctionnalité phare est LangGraph pour créer des agents cycliques.\n",
        "\n",
        "Question : {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt_rag = PromptTemplate.from_template(template_contexte)\n",
        "\n",
        "# On réutilise le même modèle 'llm' défini plus haut\n",
        "rag_chain = prompt_rag | llm\n",
        "\n",
        "print(rag_chain.invoke({\"question\": \"Qui a lancé LangChain ?\"}).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_CyV4zL7D-L"
      },
      "source": [
        "## Few-shot prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-Wti_j97Nsp",
        "outputId": "76256877-e43c-4cd4-c591-3894f4b8b295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42. Mais comme le SAV de l’existence ne décroche jamais, on bricole.\n",
            "\n",
            "Plus sérieusement: le sens, c’est un mix de\n",
            "- Lien: aimer des gens sans mode d’emploi.\n",
            "- Curiosité: apprendre des trucs inutiles… puis les utiliser.\n",
            "- Contribution: laisser un endroit un peu mieux qu’on l’a trouvé.\n",
            "- Joie: savourer les petites victoires et le bon café.\n",
            "\n",
            "Choisis-en deux pour aujourd’hui, recommence demain. Et bois de l’eau.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "\n",
        "# 1. Définition de nos exemples\n",
        "exemples = [\n",
        "    {\"input\": \"Comment ça va ?\", \"output\": \"Je ne peux pas me plaindre, mais je le fais quand même.\"},\n",
        "    {\"input\": \"Quelle heure est-il ?\", \"output\": \"L'heure de s'acheter une montre.\"},\n",
        "]\n",
        "\n",
        "# 2. Création du format pour les exemples\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 3. Injection des exemples dans le prompt final\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=exemples,\n",
        ")\n",
        "\n",
        "# 4. Assemblage du prompt final\n",
        "final_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Tu es un assistant sarcastique et plein d'esprit.\"),\n",
        "        few_shot_prompt, # On insère les exemples ici\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 5. Création et exécution de la chaîne\n",
        "sarcastic_chain = final_prompt | llm\n",
        "\n",
        "print(sarcastic_chain.invoke({\"input\": \"Quel est le sens de la vie ?\"}).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t8bN4o37ABf"
      },
      "source": [
        "# Concept 2 : construire un RAG avec Langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CtI2ykr-ZK7"
      },
      "source": [
        "## Étape 1 : Indexation (préparation des données)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyXh3cuZ7AgO"
      },
      "outputs": [],
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# 1. Chargement : On récupère le contenu d'un article Web\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=\"post-content\"))\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "# 2. Découpage : On coupe le texte en morceaux de 1000 caractères\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# 3. Stockage Vectoriel : On transforme le texte en vecteurs et on indexe dans Chroma\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "\n",
        "# On crée un \"retriever\" : l'outil capable de faire la recherche\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKVwh5iI-efe"
      },
      "source": [
        "## Étape 2 : La chaîne de génération (LCEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbYoAJu--hPH"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Initialisation du modèle (Le dernier cri : GPT-5)\n",
        "llm = ChatOpenAI(model_name=\"gpt-5\")\n",
        "\n",
        "# Définition du prompt système spécial RAG\n",
        "template = \"\"\"Vous êtes un assistant de recherche expert.\n",
        "Utilisez le contexte suivant pour répondre à la question.\n",
        "Si vous ne connaissez pas la réponse, dites simplement que vous ne savez pas.\n",
        "Soyez concis.\n",
        "\n",
        "Contexte : {context}\n",
        "\n",
        "Question : {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Petite fonction utilitaire pour joindre les documents trouvés en un seul texte\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# LA MAGIE DU LCEL (La chaîne RAG complète)\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsSDSa1u-jM6"
      },
      "source": [
        "## Étape 3 : L'exécution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfw3dsZ0-ltD",
        "outputId": "2cea4055-2448-45ad-cd36-9742a18929d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La décomposition des tâches est la pratique qui consiste à fractionner une tâche complexe en sous-tâches plus petites et gérables afin de mieux planifier et raisonner. Dans les agents autonomes, elle se fait notamment via:\n",
            "- Chain of Thought (penser étape par étape) pour dérouler une suite d’étapes simples.\n",
            "- Tree of Thoughts, qui explore plusieurs pistes de raisonnement à chaque étape (arbre de pensées, recherche BFS/DFS avec évaluation).\n",
            "- Des invites dédiées (“Steps for X…”, “Subgoals…”) ou des instructions spécifiques (ex. “écrire un plan”), voire des apports humains.\n",
            "\n",
            "Objectif: faciliter la planification long-horizon, améliorer la performance et rendre le raisonnement plus interprétable. (À distinguer de LLM+P, qui délègue la planification à un planificateur externe via PDDL.)\n"
          ]
        }
      ],
      "source": [
        "# On pose une question sur le contenu spécifique de l'article\n",
        "response = rag_chain.invoke(\"Qu'est-ce que la décomposition des tâches dans les agents autonomes ?\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmZpfZLu7A0D"
      },
      "source": [
        "# Concept 3 : agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GOwFZ8qCmgH"
      },
      "source": [
        "## Création des outils (les super-pouvoirs de l'agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeCrt-9dClza"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def multiplier(a: float, b: float) -> float:\n",
        "    \"\"\"Multiplie deux nombres. Utile pour les calculs précis.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "@tool\n",
        "def compter_lettres(mot: str) -> int:\n",
        "    \"\"\"Compte le nombre de lettres dans un mot.\"\"\"\n",
        "    return len(mot)\n",
        "\n",
        "tools = [multiplier, compter_lettres]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Mjyb64CtLq"
      },
      "source": [
        "## Initialisation de l'agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9gDpnq8CzUb",
        "outputId": "11f2f418-d71a-4b0f-e100-9cb3aefb1103"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3404887534.py:8: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  agent_executor = create_react_agent(llm, tools)\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# On instancie le modèle (GPT-5 ou GPT-4o)\n",
        "llm = ChatOpenAI(model=\"gpt-5\", temperature=0)\n",
        "\n",
        "# On crée l'agent : on lui donne le cerveau (LLM) et les mains (tools)\n",
        "agent_executor = create_react_agent(llm, tools)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-vvJ_EsC1dT"
      },
      "source": [
        "## Testons l'agent en action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eufbVE8ZC0te",
        "outputId": "c0f96c14-c6d4-4d24-ac6d-ea333ff7dcc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- 4589 × 12,5 = 57 362,5\n",
            "- Le mot \"Anticonstitutionnellement\" compte 25 lettres.\n"
          ]
        }
      ],
      "source": [
        "query = \"Combien font 4589 fois 12.5 ? Et combien de lettres compte le mot 'Anticonstitutionnellement' ?\"\n",
        "\n",
        "# L'agent va exécuter une boucle de réflexion\n",
        "response = agent_executor.invoke({\"messages\": [(\"human\", query)]})\n",
        "\n",
        "# Affichons la réponse finale\n",
        "print(response[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQScqTCkC5d4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "f30pTxEr66rF",
        "6QAqsjQm6yjb",
        "9UojOidS607k",
        "C_CyV4zL7D-L",
        "6t8bN4o37ABf",
        "TmZpfZLu7A0D"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
